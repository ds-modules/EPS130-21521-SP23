{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework1: Earthquake Occurrence Statistics\n",
    "\n",
    "The statistics of earthquake occurrence is revealed from catalogs of seismicity, which include event time, location and magnitude. We will talk about how earthquakes are located and how magnitudes are estimated separately, but for now it is sufficient to know that this information can be easily acquired. With such catalogs it is possible to compare the seismic activity of different regions, make informed assessments about the frequency of earthquake occurrence, and learn about the fault rupture process. Maps of the earthquakes in catalogs over time reveal the structure of faulting in a region, and provide a framework with which to study the seismotectonics of a region.\n",
    "\n",
    "There are two primary earthquake statistics used by seismologists. They are the Gutenberg-Richter relationship (Gutenberg and Richter, 1949), and the Omori Law (Omori, 1894). \n",
    "\n",
    "Gutenberg and Richter found that when the logarithm of the number of earthquakes is plotted vs. magnitude that the distribution (data) may be described by linear model, log(N)=A+Bm, where N is the number of earthquakes equal to or larger than m, m is the magnitude and A (y-intercept) and B (slope) are refered to as the Gutenberg-Richter statistics or coefficients. They found that on a global scale, and subsequently more generally, the B-value or the slope of the Gutenberg-Richter line is approximately equal to -1. Thus for each increase in earthquake magnitude there are approximately 10 times fewer earthquakes. If, for example, there are 100 M3 events in a region per year, then the Gutenberg-Richter relationship generally finds that there would be approximately 10 M4 events and 1 M5 event in each year. For magnitudes larger than M5 there would be fewer than one event per year. Gutenberg-Richter is a very important earthquake statistic because it is used to determine the rates of earthquake occurrence, which is a key step in characterizing earthquake hazards (we will see this in future homework exercises). \n",
    "\n",
    "The Omori Law is used to characterize the rate at which aftershocks occur following a large mainshock event. This statistic is used for comparing the aftershock productivity of different earthquakes and regions, make forecasts of the likelihood of large damaging aftershocks and to distinguish between earthquake faulting and possibly geothermal or volcanic-related seismicity by examining whether the distribution describes a \"mainshock/aftershock\" pattern or is more \"swarm-like\". \n",
    "\n",
    "In this homework you will use python code in this notebook to investigate the Gutenberg-Richter and Omori statistics for the San Francisco Bay Area, as well as develop numerical analysis skills using python. \n",
    "\n",
    "Note: This is not a python class, but the primary programming tool that we will be used is python. However, if you know MatLab or have other programing background and would prefer to use it, you are free to use those tools instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Initial Setup and Subroutine Definitions - No need to edit this cell\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader as cReader\n",
    "import pandas as pd\n",
    "\n",
    "#The following are useful functions/subroutines\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two geographic points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371.0 * c\n",
    "    return km\n",
    "\n",
    "def parseCatalog(p):\n",
    "    '''\n",
    "    Function to slice an ANSS catalog loaded as a pandas dataframe and return arrays of info, including a days array\n",
    "    '''\n",
    "    year=p['DateTime'].dt.year\n",
    "    month=p['DateTime'].dt.month\n",
    "    day=p['DateTime'].dt.day\n",
    "    hour=p['DateTime'].dt.month\n",
    "    minute=p['DateTime'].dt.minute\n",
    "    sec=p['DateTime'].dt.second\n",
    "    lat=p['Latitude'].values\n",
    "    lon=p['Longitude'].values\n",
    "    mag=p['Magnitude'].values\n",
    "\n",
    "    days = countDays(len(year),year,month,day)\n",
    "    return year,month,day,hour,minute,sec,lat,lon,mag,days\n",
    "\n",
    "def countDays(npts,y,m,d):\n",
    "    '''\n",
    "    Function to create an array of days\n",
    "    '''\n",
    "    days=np.zeros(npts)\n",
    "    for i in range(0,npts,1):\n",
    "        d0 = datetime.date(y[0], m[0], d[0])\n",
    "        d1 = datetime.date(y[i], m[i], d[i])\n",
    "        delta = d1 - d0\n",
    "        days[i]=delta.days\n",
    "    return days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Catalog\n",
    "We have downloaded the Advanced National Seismic System (ANSS) catalog from 1900 to 2020 for you to use. This catalog has all events in the aforementioned time range located within 100 km of UC Berkeley. Columns of this catalog include information about the catalogued earthquakes, including the date and time of each event, its location in latitude, longitude and depth, and the event magnitude.  \n",
    "\n",
    "The following python code reads this catalog file and places the information in arrays for analysis.\n",
    "\n",
    "You can get catalogs from various sources such as:\n",
    "\n",
    "    NCEDC: https://www.ncedc.org/ncedc/catalog-search.html\n",
    "\n",
    "    USGS: https://www.usgs.gov/programs/earthquake-hazards/earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "bay_catalog=pd.read_csv('data/bay_area_anss_1900_2020.csv')\n",
    "\n",
    "\n",
    "#There are NaN in magnitude - lets purge those\n",
    "bay_catalog=bay_catalog.dropna(subset=['Magnitude'])  #this drops the NaN from the Magnitude columns\n",
    "bay_catalog=bay_catalog.reset_index(drop=True)        #this resets the index for the updated pandas dataframe\n",
    "\n",
    "\n",
    "bay_catalog['DateTime'] = pd.to_datetime(bay_catalog['DateTime'])  #this converts the format to datetime which has some nice functions\n",
    "\n",
    "\n",
    "bay_catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data arrays - take a look at the function parseCatalog() above to see what it is doing\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(bay_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Explore the raw catalog (10 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print basic catalog info\n",
    "Find the number of events, the number of days from the first event, the minimum magnitude, and the maximum magnitude. Use the variable names provided so that the example print statements will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Finish this code\n",
    "nevt = ...\n",
    "ndays = ... \n",
    "min_mag = ... \n",
    "max_mag = ... \n",
    "\n",
    "#Example print statements\n",
    "#print(f'There are {nevt} events in the catalog.')\n",
    "#print(f'The first event was {ndays:.0f} days before the last.')\n",
    "#print(f'The magnitudes range from {min_mag} to {max_mag}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the catalog time series\n",
    "Make an x-y scatter plot showing the magnitude of the earthquake on the y-axis and the time of the event on the x-axis. For this it is convenient that the last output of the `readAnssCatalog` function is the number of days since the beginning of the catalog. The plot will show that the catalog is not uniform due to the fact that over time as more seismic recording stations were installed more earthquakes could be detected and properly located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now the plot\n",
    "#These are the basic plotting commands - there are many more functions and features that can be used\n",
    "plt.plot(days, mag,'o',alpha=0.2,markersize=5)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Raw Event Catalog')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the Raw Earthquake Catalog\n",
    "\n",
    "On a map of the Bay Area plot the location of events in the declustered catalog. Scale the marker color and size by magnitude. The initial map with faults plotted is below. To plot the seismicity you will want to use `plt.scatter()` and `plt.plot()` calls. Note that if you type `plt.scatter?` in a cell you will get the documentation for the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First read in the fault data - No need to edit this cell\n",
    "\n",
    "a=pd.read_table('data/Hayward.txt') \n",
    "hay_lon=a['x'].values\n",
    "hay_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Andreas.txt') \n",
    "SA_lon=a['x'].values - 0.03 #adjustment to the west\n",
    "SA_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/San_Gregorio.txt') \n",
    "SG_lon=a['x'].values\n",
    "SG_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Calaveras.txt') \n",
    "cal_lon=a['x'].values\n",
    "cal_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Hunting_Creek.txt') \n",
    "HC_lon=a['x'].values\n",
    "HC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Rodgers_Creek.txt') \n",
    "RC_lon=a['x'].values\n",
    "RC_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Concord.txt') \n",
    "con_lon=a['x'].values\n",
    "con_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Greenville.txt') \n",
    "grn_lon=a['x'].values\n",
    "grn_lat=a['y'].values\n",
    "\n",
    "a=pd.read_table('data/Maacama.txt') \n",
    "m_lon=a['x'].values\n",
    "m_lat=a['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of the earthquake catalog\n",
    "\n",
    "# Set Corners of Map\n",
    "lat0=37.0\n",
    "lat1=38.75\n",
    "lon0=-123.25\n",
    "lon1=-121.5\n",
    "tickstep=0.25 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks, crs=ccrs.PlateCarree())\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "#Sort Descending to plot largest events on top - finish the code\n",
    "indx=np.argsort(mag)   #determine sort index\n",
    "x=...            #apply sort index\n",
    "y=...            #apply sort index\n",
    "z=...            #exponent to scale size\n",
    "\n",
    "#Modify the plt.scatter() function to plot the earthquakes\n",
    "#plt.scatter(...)\n",
    "\n",
    "\n",
    "plt.plot(hay_lon,hay_lat,'-',color='red',linewidth=2,label='Hayward Fault')\n",
    "plt.plot(SA_lon,SA_lat,'-',color='orange',linewidth=2,label='San Andreas Fault')\n",
    "plt.plot(SG_lon,SG_lat,'-',color='yellow',linewidth=2,label='San Gregorio Fault')\n",
    "plt.plot(cal_lon,cal_lat,'-',color='green',linewidth=2,label='Calaveras Fault')\n",
    "plt.plot(con_lon,con_lat,'-',color='cyan',linewidth=2,label='Concord Fault')\n",
    "plt.plot(grn_lon,grn_lat,'-',color='lime',linewidth=2,label='Greenville Fault')\n",
    "plt.plot(m_lon,m_lat,'-',color='blueviolet',linewidth=2,label='Maacama Fault')\n",
    "plt.plot(RC_lon,RC_lat,'-',color='magenta',linewidth=2,label='Rodgers Creek Fault')\n",
    "plt.plot(HC_lon,HC_lat,'-',color='blue',linewidth=2,label='Hunting Creek Fault')\n",
    "plt.plot(Berk_lon,Berk_lat,'rs',markersize=8,label='UC Berkeley')  # plot red square on Berkeley Campus\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**:\n",
    "\n",
    "1. Describe the seismicity and any patterns that you see.\n",
    "\n",
    "2. How well does the seismicity show the region's major faults?\n",
    "\n",
    "3. Reviewing fault maps online what faults missing from the plot might be added to better explain the seismicity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Compute the Gutenberg-Richter statitistics (30 pts)\n",
    "\n",
    "Follow the steps below to compute the Gutenberg Richter statistics for the raw catalog.\n",
    "\n",
    "- Define a magnitude bin array (m) using `np.arange()`. Considering m from 0 to the maximium magnitude in the catalog with 0.1 steps will be sufficient.\n",
    "\n",
    "- Next,  write a loop that counts all events greater than equal to a given magnitude bin, and places it in a array, N, with the same length as m. You can use the `np.count_nonzero()` function for this.\n",
    "\n",
    "- Commonly the the number of events in each bin, N, is divided by the number of years the catalog spans to give the annual number of events per year. The result is the annual number of events for each magnitude (m) bin.\n",
    "\n",
    "- Then lake the `np.log10()` of N and for clarity you can name the array logN.\n",
    "\n",
    "- Plot logN vs. m, where LogN is on the y-axis and m is on the x-axis.\n",
    "\n",
    "- The Gutenberg-Richter relationship is linear in logN - m space. Does the catalog data agree with this assumption? Is there a range of m where there is better agreement with the linear model?\n",
    "\n",
    "- What is the reason for the different trend at small magnitude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Gutenberg-Richter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of mag bins with width 0.1 magnitude per bin\n",
    "m=np.arange(...) \n",
    "\n",
    "# Preallocate the vector logN with a zeros vector of size len(m)\n",
    "logN = np.zeros(len(m))\n",
    "ny = (max(days)-min(days))/365 \n",
    "\n",
    "# Find N\n",
    "for i in range(0,len(m),1):\n",
    "    logN[i]= np.log10(np.count_nonzero(...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "#code for plot here\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the data to find the Gutenberg Richter statistics.\n",
    "\n",
    "Now, fit the data with the Gutenberg Richter relationship $log_{10}(N(m))$=A+Bm. In other words, \"invert\" the data to find the applied model parameters. Since the model is linear we can use one of the methods to fit a line to the data. `np.polyfit()` is suitable function for this.\n",
    "\n",
    "To get better results limit the m, logN arrays to consider only the portion of the data that has a linear behavior, and compare with the data. `np.polyval()` can be used to create the model series. You can overlay the modeled line with the data either in a single `plt.plot()` function, or through sequential calls to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the model fit using np.polyfit() and use np.polyval() to generate the model points (yfit)\n",
    "x=m[..]\n",
    "y=logN[..]\n",
    "p=np.polyfit(...,...,1)\n",
    "yfit=np.polyval(...,...)\n",
    "\n",
    "#Plot comparing input data as points and model as a red line\n",
    "\n",
    "#Print model coefficients\n",
    "print(f'A={p[1]:0.3f} B={p[0]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. Discuss the quality of the fit of the Gutenberg-Richter model.\n",
    "\n",
    "2. Given the Gutenberg-Richter parameters you found, what is the annual frequncy of M5, 6 and 7 earthquakes. What is the average recurrence (interval) time of these earthquakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your work to estimate annual frequency and recurrence intervals here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Declustering (30 pts)\n",
    "\n",
    "In the above analysis mainshocks (primary events) and aftershocks are mixed together. The results for the Gutenberg-Richter statistics were generally pretty good, however a correct implementation of Gutenberg-Richter considers only the primary events. Therefore, we seek a catalog with aftershocks removed in order to improve our assessment of the Gutenberg-Richter statistics. The process to remove aftershocks is called declustering.\n",
    "\n",
    "In this exercise, you will evaluate a published declustering method as you use it to decluster the catalog analyzed above. Then you will re-compute the Gutenberg-Richter coefficients for the declustered catalog in order to examine the affect on the G-R statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declustering Algorithm\n",
    "\n",
    "The analysis that was just performed was for the raw catalog, which means that it includes all events. However Gutenberg-Richter is really interested in the occurrence of primary \"main shock\" events, and therefore it is necessary to decluster the catalog to obtain an unbiased estimate of the G-N coefficients. Declustering here means remove the aftershocks from the catalog. This is done using an algorithm that relates the \"expected\" time and distance range of aftershocks from a given mainshock. Large mainshocks will result in aftershock populations that, statistically speaking, have a greater likelihood to occur over longer time periods and greater mainshock-aftershock distances compared with smaller mainshock-aftershock series. \n",
    "\n",
    "The code block below defines a declustering algorithm. This algorithm uses distance and time metrics that are magnitude dependent, called 'Dtest' and 'Ttest'. If a given event falls within the maximal values defined by Dtest and Ttest for its magnitude it is deemed an aftershock and removed from the catalog. After all events are processed, the remaining catalog is then comprised of only primary events. This declustered catalog can be used to estimate more accurate Gutenberg-Richter statistics. Furthermore, we can study the aftershock events that the algorithm removed for a given earthquake in the context of the Omori Law statistics (Exercise 4).\n",
    "\n",
    "Because aftershock identification is an empirical procedure, there are many different ways to define the Dtest and Ttest relationships. Stiphout et al., (2012, on page 10) summarizes three different definitions of the Dtest/Ttest relationships originally proposed by Uhrhammer (1986), Knopoff and Gardner (1972). The following are the equations in Stiphout et al. (2012) are for method 1 in the declustering function defined below.\n",
    "<img src=\"./Figures/window_approx.png\">\n",
    "\n",
    "Compare the event reduction rate (final number divided by the initial number of events) for the three different proposed distance and time windows. You can do this by adding a logical (if statement) tree to enable switching between different definitions of Dtest and Ttest in declustering_algorithm below. The first definition (Eqn 1 from Stiphout et al., 2012, p.10) has already been completed. Note that some of these algorithms will not run instantaneously and might take up to a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declustering_algorithm(cat, definition=1):\n",
    "    '''\n",
    "    Decluster a catalog\n",
    "    \n",
    "    note: This function may take a few minutes to complete\n",
    "    \n",
    "    calls parseCatalog()\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    cat must be an anss formatted pandas datafram\n",
    "    definition is the algorithm (1 - 2) from Stiphout, 2012, which determines Dtest and Ttest values\n",
    "        Definition = 1 : Gardner and Knopoff, 1974 [default]\n",
    "        Definition = 2 : Uhrhammer, 1986\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # do not edit\n",
    "    cnt=0\n",
    "    save=np.zeros((1,10000000),dtype=int)\n",
    "\n",
    "    # grab catalog arrays\n",
    "    year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(cat)\n",
    "    ne=len(year)\n",
    "\n",
    "    # main for-loop over events\n",
    "    for i in range(0,ne,1):\n",
    "        \n",
    "        if definition == 1:\n",
    "            \n",
    "            # Definition #1 : Knopoff and Gardner, 1972\n",
    "            Dtest=np.power(10,0.1238*mag[i]+0.983)\n",
    "            if mag[i] >= 6.5:\n",
    "                Ttest=np.power(10,0.032*mag[i]+2.7389)\n",
    "            else:\n",
    "                Ttest=np.power(10,0.5409*mag[i]-0.547)\n",
    "  \n",
    "        elif definition == 2:\n",
    "\n",
    "            # Definition #2 : Uhrhammer, 1986\n",
    "            Dtest=np.exp(-1.204+0.804*mag[i]) # SOLUTION\n",
    "            Ttest=np.exp(-2.87+1.235*mag[i]) # SOLUTION\n",
    "\n",
    "            \n",
    "            \n",
    "        a=days[i+1:ne]-days[i]\n",
    "        m=mag[i+1:ne]\n",
    "        b=haversine_np(lon[i],lat[i],lon[i+1:ne],lat[i+1:ne])\n",
    "\n",
    "        icnt=np.count_nonzero(a <= Ttest)\n",
    "        if icnt > 0:\n",
    "            itime=np.array(np.nonzero(a <= Ttest)) + (i+1)\n",
    "            for j in range(0,icnt,1):             \n",
    "                if b[j] <= Dtest and m[j] < mag[i]:\n",
    "                    save[0][cnt]=itime[0][j]\n",
    "                    cnt += 1 # save contains index of aftershocks in cat\n",
    "\n",
    "    #Note this is an array of indexes that will be used to delete events flagged \n",
    "                        #as aftershocks\n",
    "    save=np.delete(np.unique(save),0)  \n",
    "    \n",
    "    # Filter or slice out the declustered and aftershock dataframe catalogs from the \n",
    "    # original dataframe catalog \"data\" using \"save\" above.\n",
    "    cat_aftershocks = cat.iloc[np.unique(save)] \n",
    "\n",
    "    cat_declustered = cat.iloc[~cat.index.isin(save)]\n",
    "    \n",
    "    cat_aftershocks.reset_index(drop=True, inplace=True)\n",
    "    cat_declustered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cat_declustered, cat_aftershocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the declustering algorithm\n",
    "method = 1\n",
    "data=bay_catalog\n",
    "data_declustered, data_aftershocks = declustering_algorithm(data, definition=method)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "# The sum of the aftershocks plus the primary events should equal the length of the original catalog\n",
    "\n",
    "#The sum of the aftershocks and the primary events should equal the original length of the data. The following line should return \"True\"\n",
    "len(data) == len(data_aftershocks) + len(data_declustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a map showing the declustered catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First set variables for declustered catalog using parseCatalog()\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(data_declustered)\n",
    "\n",
    "#Code for map here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Re-compute the Gutenberg-Richter statistics as above for the declustered catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code comparing the Gutenberg-Richter parameters, and the fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- How many events were removed from the catalog with each declustering algorithm?\n",
    "\n",
    "- How do the Gutenberg-Richter A and B values differ using the two declustering algorithms?\n",
    "\n",
    "- Compare the estimated recurrence intervals for M6 and M7 earthquakes.\n",
    "\n",
    "- Compare your estimated recurrence values with what has been presented in the USGS Earthquake Hazard Assessments for the return period for Hayward fault earthquakes. The Hayward fault data can be found from https://www.usgs.gov/news/hayward-fault-it-due-a-repeat-powerful-1868-earthquake?qt-news_science_products=3#qt-news_science_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Omori Law for Loma Prieta M6.9 Event (30 pts)\n",
    "\n",
    "Here we will use the declustering algorithm to identify aftershocks of the October 18 1989 at 04:15am (October 17 at 5:15pm PDT) the M6.9 Loma Prieta earthquake occurred in the Santa Cruz mountains approximately 80 km southwest of the Berkeley Campus. This wiki has some background information for the earthquake: https://en.wikipedia.org/wiki/1989_Loma_Prieta_earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Earthquake Catalog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "LP_catalog=pd.read_csv('data/bay_area_anss_1989_1995.csv')\n",
    "\n",
    "LP_catalog=LP_catalog.dropna(subset=['Magnitude'])  #this drops the NaN from the Magnitude columns\n",
    "LP_catalog=LP_catalog.reset_index(drop=True)\n",
    "\n",
    "#the following function just reformats to a DateTime object format\n",
    "LP_catalog['DateTime'] = pd.to_datetime(LP_catalog['DateTime'])\n",
    "LP_catalog.head()\n",
    "\n",
    "#parse catalog into arrays\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(LP_catalog)\n",
    "LP_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the LP catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat0=36.0\n",
    "lat1=38.5\n",
    "lon0=-123.0\n",
    "lon1=-121.0\n",
    "tickstep=0.5 #for axes\n",
    "latticks=np.arange(lat0,lat1+tickstep,tickstep)\n",
    "lonticks=np.arange(lon0,lon1+tickstep,tickstep)\n",
    "\n",
    "# coordinates for UC Berkeley\n",
    "Berk_lat = 37.8716\n",
    "Berk_lon = -122.2727\n",
    "\n",
    "plt.figure(1,(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([lon0, lon1, lat0, lat1], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='10m',linewidth=1)\n",
    "ax.set_xticks(lonticks)\n",
    "ax.set_yticks(latticks)\n",
    "ax.set(xlabel='Longitude', ylabel='Latitude',title='Raw Catalog')\n",
    "\n",
    "# Sort by magnitude to plot largest events on top\n",
    "LP_catalog_sorted = LP_catalog.sort_values(by=['Magnitude'])\n",
    "#exponent to scale marker size\n",
    "z=np.exp(LP_catalog_sorted['Magnitude'])    \n",
    "\n",
    "plt.scatter(LP_catalog_sorted['Longitude'], LP_catalog_sorted['Latitude'], s=z, \n",
    "            c=LP_catalog_sorted['Magnitude'], cmap='plasma',alpha=0.4,marker='o') # plot circles on EQs\n",
    "plt.plot(Berk_lon,Berk_lat,'s',color='limegreen',markersize=8)  # plot red square on Berkeley Campus\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse LP_catalog for 3 months from the earthquake & create arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can parse the catalog in different ways. Using boolean logic in a series of filters, or more simply making use\n",
    "#of the pandas between function, e.g. dataframe.between(). The example below filters 4 months of data beginning at\n",
    "#the time of the mainshock\n",
    "\n",
    "#First lets make a deep copy of the data\n",
    "LPEQ=LP_catalog.copy(deep=True)\n",
    "\n",
    "start_date=pd.datetime(1989,10,18,0,4)\n",
    "end_date=pd.datetime(1990,2,18,0,0)\n",
    "LPEQ=LPEQ[LPEQ['DateTime'].between(start_date,end_date)]\n",
    "LPEQ=LPEQ.reset_index(drop=True)\n",
    "\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(LPEQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Loma Preita time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude vs. days from mainshock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Loma Preita Earthquake Catalog in map view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for LP sequence map here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decluster the Raw Catalog for the Loma Prieta time period\n",
    "\n",
    "We use the same decluster algorithm previously to identify aftershocks and remove them from the 30-day Loma Preita catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declustering code here\n",
    "data_dec, data_after = declustering_algorithm(...,...)\n",
    "\n",
    "# This condition should print out \"True\" if the catalogs were separated correctly\n",
    "len(LPEQ) == len(data_after) + len(data_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two new sets of event info arrays, one for the declustered catalog and one for the aftershock catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyear,dmonth,dday,dhour,dminute,dsec,dlat,dlon,dmag,ddays = parseCatalog(data_dec)\n",
    "ayear,amonth,aday,ahour,aminute,asec,alat,alon,amag,adays = parseCatalog(data_after)\n",
    "dnevt =len(ddays)\n",
    "anevt=len(adays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Aftershock Catalog in time series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the declustered Loma Prieta mainshock catalog in map view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make a Map of the declustered events\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the declustered Loma Prieta aftershock catalog in map view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Map of Aftershock events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omori statistics\n",
    "\n",
    "To compute the Omori statistics we want to bin the log10 of the number of aftershocks each day following the mainshock and fit a power law equation such as:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "N=\\frac{A}{(t+\\epsilon)^P}, \n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "where t is time in days, N is the number of earthquakes in the 24 hour period, and $\\epsilon$ is a small number (fraction of a day) to avoid the singularity at zero time. A and P are the coeffients that we want to find through regression. This power law equation can be linearized by simply taking the log10 of both sides giving:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "log_{10}(N)=a + P*log_{10}(t+\\epsilon)\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Note: a=log10(A). Also when we regress the data, fit a line, the sign of P will be determined for us.\n",
    "\n",
    "Note: We will use both the Gutenberg-Richter and the Omori Law statistics computed in Homework 1 in Homework 2 where we will examine the probability of earthquake occurrence and aftershock occurrence following a given mainshock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the Omori Data and plot it\n",
    "epsilon=0.1\n",
    "maxdays=int(np.max(adays))\n",
    "t=np.arange(1,maxdays+2,1)\n",
    "\n",
    "logt=np.log10(...)\n",
    "N=np.zeros(...)\n",
    "\n",
    "#Loop to count events for each day using np.count_nonzero()\n",
    " \n",
    "logN=np.log10(...)\n",
    "\n",
    "#Code for plot here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a line to the logN and logt data and then compare in N-t space\n",
    "p=np.polyfit(...,...,1)\n",
    "logNN=np.polyval(...,...)\n",
    "NN=10**logNN\n",
    "\n",
    "#Plot comparing the data (blue dots) and model(NN) (red line)\n",
    "\n",
    "\n",
    "#Print the a and P coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Which faults were active during Loma Prieta?\n",
    "\n",
    "- What could cause aftershocks to occur on faults other than the mainshock fault?\n",
    "\n",
    "- How well do the Loma Prieta aftershocks follow the Omori Power Law?\n",
    "\n",
    "- How do the results compare using method=2 to decluster the catalog?\n",
    "\n",
    "- How well does the estimated P value compare to the range of values given in Lay and Wallace?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets use the Pre-Loma Prieta daily seismicity rate to assess when the Loma Prieta sequence aftershocks end (trend into the pre-event rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a new deep copy of the LP_catalog\n",
    "PRELP=LP_catalog.copy(deep=True)\n",
    "\n",
    "#Parse catalog having the same number of months as the LPEQ catalog\n",
    "start_date=pd.datetime(...)\n",
    "end_date=pd.datetime(...)\n",
    "PRELP=PRELP[LP_catalog['DateTime'].between(start_date,end_date)]\n",
    "PRELP=PRELP.reset_index(drop=True)\n",
    "\n",
    "year,month,day,hour,minute,sec,lat,lon,mag,days = parseCatalog(PRELP)\n",
    "PRELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First compute the number of events per day in the PRELP catalog\n",
    "\n",
    "#Compute the mean number of earthquakes per day\n",
    "meanN=np.mean(preN)\n",
    "print(f'mean pre-LP events per day {meanN:.1f}')\n",
    "\n",
    "#Use np.where() to find the index where N is less than the pre-LP daily rate.\n",
    "#Use the first value of the index array returned by np.where to determine the numbers of days elaspsed when the seismicity\n",
    "#rate first returns to the pre-LP average\n",
    "\n",
    "result=np.where(...)  #N is the LP sequence daily bins, and meanN is the average from the pre-LP catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Based on the daily rate of earthquakes when does the Loma Prieta earthquake aftershock sequence \"end?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Save the completed notebook as a `pdf` and upload to becourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
